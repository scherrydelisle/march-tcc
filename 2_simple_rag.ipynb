{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1eefa2c-8206-4d40-9f18-500007928069",
   "metadata": {},
   "source": [
    "### **Very Simple RAG**  \n",
    "\n",
    "We will use a very basic retrieval-augmented generation (RAG) approach. The idea is:  \n",
    "\n",
    "1. The user performs a query.  \n",
    "2. We take the query from (1) and perform an additional lookup to gather context.  \n",
    "3. We take the query from (1) and the context from (2) to ask the LLM.  \n",
    "\n",
    "In most applications and use cases, customer data is confidential, and the LLM has no prior knowledge of it. Furthermore, recent changes or factual data require a reference as a guide.  \n",
    "\n",
    "In RAG, for step (2), we typically query a vector database, Elasticsearch, or other external services to retrieve context. Additionally, depending on the user's role and resource permissions, we refine the results further to prevent privileged information from being leaked.  \n",
    "\n",
    "From 2023 to early 2024, most SaaS providers began incorporating RAG into their search and reporting tools to surface relevant information. Many startups also introduced capabilities to extract context from PDFs—for example, companies analyzing construction bids.  \n",
    "\n",
    "---\n",
    "\n",
    "### Ranking  \n",
    "\n",
    "- Yes, still think about ranking—BM25 and similar techniques remain relevant.  \n",
    "- Also, consider how you would validate performance. How would you measure CTR@1, CTR@5, and other key metrics?  \n",
    "\n",
    "Nowadays, ranking is less critical if you don't require an immediate response.  \n",
    "\n",
    "---\n",
    "\n",
    "### Authorization  \n",
    "\n",
    "- How would you ensure that privileged information is not exposed?  \n",
    "---\n",
    "\n",
    "### **RAG Use Cases**  \n",
    "\n",
    "These are very simple to implement! You could build them in a day or so.  \n",
    "\n",
    "- How would you design a service that automatically identifies **cheaper deals** or **artsy alternatives** based on the webpage you're browsing?  \n",
    "- How would you build a **search engine** for permits or a city-wide **311 bot**?  \n",
    "- How could we help a city's 311 operators with training or improve their efficiency? Could we use live transcription services to surface relevant content?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d9bf93-7a8c-43a3-9532-5997179c9aaf",
   "metadata": {},
   "source": [
    "### Quiz  \n",
    "\n",
    "We will build a basic RAG system that can answer questions about movies.  \n",
    "\n",
    "1. The user performs a query.  \n",
    "2. We take the query from (1) and retrieve relevant context.  \n",
    "3. We select a few results from (2) and create a prompt.  \n",
    "4. We ask the LLM to answer the question.  \n",
    "\n",
    "This is the simplest example, but notice how the request flow remains the same, whether you're building RAG for **Cineplex booking info**, a **legal firm**, or any other domain.  \n",
    "\n",
    "The core idea—performing an internal search, ranking the results, and adding relevant context to our prompt—remains unchanged!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5578f0ab-fe5e-49db-a9bb-c856315b7442",
   "metadata": {},
   "outputs": [],
   "source": [
    "import anthropic\n",
    "from utils import chunk_text, ANTHROPIC_API_KEY, visualize_citations\n",
    "\n",
    "client = anthropic.Anthropic(api_key=ANTHROPIC_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241f6c61-0dc4-4cfc-9ca5-f7954cb99b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "class SimpleRag():\n",
    "    def __init__(self):\n",
    "        self.encoder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        self.vector_store = {\n",
    "            'embeddings': [],\n",
    "            'sources': [],\n",
    "            'chunks': []\n",
    "        }\n",
    "\n",
    "    def _add_to_store(self, file: str):\n",
    "        with open(file, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "        chunks = chunk_text(content)        \n",
    "        embeddings = self.encoder.encode(chunks)\n",
    "        self.vector_store['embeddings'].extend(embeddings)\n",
    "        self.vector_store['sources'].extend([file] * len(chunks))\n",
    "        self.vector_store['chunks'].extend(chunks)\n",
    "\n",
    "    def retrieve_context(self, q: str, k: str):\n",
    "        question_embedding = self.encoder.encode([q])[0]\n",
    "        similarities = cosine_similarity([question_embedding], self.vector_store['embeddings'])[0]\n",
    "        top_k_indices = np.argsort(similarities)[-k:][::-1]\n",
    "        results = []\n",
    "\n",
    "        # example\n",
    "        # source = self.vector_store['sources'][top_k_indices[0]]\n",
    "        # content = ....\n",
    "        \n",
    "        raise NotImplementedError(\"fill me\")\n",
    "        return results\n",
    "\n",
    "    def make_prompt(self, q: str, ctxs: list[str]):\n",
    "        raise NotImplementedError(\"fill me\")\n",
    "    \n",
    "    def query(self, q: str):\n",
    "        raise NotImplementedError(\"fill me\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6fa443-2a1a-4e13-b2e7-18b8b7498c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag = SimpleRag()\n",
    "rag._add_to_store(\"docs/example.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b131bd-f506-49b8-9394-248be1de1f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Who directed movie Mickey 17, and what is it about?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2f16bc-5b2a-4609-9702-24ec4d80bd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag.query(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34814a7c-008c-445b-a642-096b89d77d00",
   "metadata": {},
   "source": [
    "## Using Citation Support\n",
    "\n",
    "Claude (our LLM) to provide detailed citations when answering questions about documents. \n",
    "Citations are a valuable affordance in many LLM powered applications to help users track and verify the sources of information in responses.\n",
    "\n",
    "https://docs.anthropic.com/en/docs/build-with-claude/citations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24aa3b90-de85-4c67-8b56-0e44a2121683",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import json\n",
    "\n",
    "pdf_path = '' # fill me\n",
    "with open(pdf_path, \"rb\") as f:\n",
    "    pdf_data = base64.b64encode(f.read()).decode()\n",
    "\n",
    "pdf_response = client.messages.create(\n",
    "    model=\"claude-3-5-sonnet-latest\",\n",
    "    temperature=0.0,\n",
    "    max_tokens=4000,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"document\",\n",
    "                    \"source\": {\n",
    "                        \"type\": \"base64\",\n",
    "                        \"media_type\": \"application/pdf\",\n",
    "                        \"data\": pdf_data\n",
    "                    },\n",
    "                    \"title\": \"\", # fill me\n",
    "                    \"citations\": {\"enabled\": True}\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"\" # fill me\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(pdf_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45483f1-5d21-458e-b31b-56dec3bf6c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(visualize_citations(pdf_response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85da0ae1-8cb9-422e-b662-e966270d0ef5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
