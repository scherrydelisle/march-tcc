{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5dbbe392-f0f4-4c86-8f94-66c7ec638c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import anthropic\n",
    "\n",
    "from utils import ANTHROPIC_API_KEY\n",
    "\n",
    "client = anthropic.Anthropic(api_key=ANTHROPIC_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "69c21a91-368d-4af8-accc-58b4374478f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete(\n",
    "    prompt: str, \n",
    "    sys_prompt: str = \"\",\n",
    "    prefill: str = \"\",\n",
    "    model_name: str = \"claude-3-5-haiku-latest\",\n",
    "    temperature: float = 0.0,\n",
    "    max_tokens: int = 4000,\n",
    "    \n",
    "):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    if prefill != \"\":\n",
    "        messages.append({\"role\": \"assistant\", \"content\": prefill})\n",
    "\n",
    "    message = client.messages.create(\n",
    "        model=model_name,\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=temperature,\n",
    "        system=sys_prompt,\n",
    "        messages=messages\n",
    "    )\n",
    "    return message.content[0].text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b4aeadef-463e-4fe1-baed-a4087ffe397e",
   "metadata": {},
   "source": [
    "### Example 1: Chain workflow\n",
    "\n",
    "This workflow is ideal for situations where the task can be easily and cleanly decomposed into fixed subtasks. The main goal is to trade off latency for higher accuracy, by making each LLM call an easier task.\n",
    "\n",
    "Each step progressively transforms raw text into a formatted table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dcda9765-59ce-4497-a2db-447e3da4c2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chain(inp, prompts: List[str], llm_fn):\n",
    "    result = inp\n",
    "    for i, prompt in enumerate(prompts, 1):\n",
    "        formatted_prompt = f\"{prompt}\\n\\n Input: {result}\"\n",
    "        result = llm_fn(formatted_prompt)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f5d68ef7-595b-442a-8479-b3a25d821c5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input text:\n",
      "\n",
      "Q3 Performance Summary:\n",
      "Our customer satisfaction score rose to 92 points this quarter.\n",
      "Revenue grew by 45% compared to last year.\n",
      "Market share is now at 23% in our primary market.\n",
      "Customer churn decreased to 5% from 8%.\n",
      "New user acquisition cost is $43 per user.\n",
      "Product adoption rate increased to 78%.\n",
      "Employee satisfaction is at 87 points.\n",
      "Operating margin improved to 34%.\n",
      "\n",
      "Here's the data formatted as a markdown table:\n",
      "\n",
      "| Metric | Value |\n",
      "|:--|--:|\n",
      "| Customer Satisfaction | 92% |\n",
      "| Employee Satisfaction | 87% |\n",
      "| Product Adoption Rate | 78% |\n",
      "| Revenue Growth | 45% |\n",
      "| User Acquisition Cost | 43.00 |\n",
      "| Operating Margin | 34% |\n",
      "| Market Share | 23% |\n",
      "| Customer Churn | 5% |\n"
     ]
    }
   ],
   "source": [
    "data_processing_steps = [\n",
    "    \"\"\"Extract only the numerical values and their associated metrics from the text.\n",
    "    Format each as 'value: metric' on a new line.\n",
    "    Example format:\n",
    "    92: customer satisfaction\n",
    "    45%: revenue growth\"\"\",\n",
    "    \n",
    "    \"\"\"Convert all numerical values to percentages where possible.\n",
    "    If not a percentage or points, convert to decimal (e.g., 92 points -> 92%).\n",
    "    Keep one number per line.\n",
    "    Example format:\n",
    "    92%: customer satisfaction\n",
    "    45%: revenue growth\"\"\",\n",
    "    \n",
    "    \"\"\"Sort all lines in descending order by numerical value.\n",
    "    Keep the format 'value: metric' on each line.\n",
    "    Example:\n",
    "    92%: customer satisfaction\n",
    "    87%: employee satisfaction\"\"\",\n",
    "    \n",
    "    \"\"\"Format the sorted data as a markdown table with columns:\n",
    "    | Metric | Value |\n",
    "    |:--|--:|\n",
    "    | Customer Satisfaction | 92% |\"\"\"\n",
    "]\n",
    "\n",
    "report = \"\"\"\n",
    "Q3 Performance Summary:\n",
    "Our customer satisfaction score rose to 92 points this quarter.\n",
    "Revenue grew by 45% compared to last year.\n",
    "Market share is now at 23% in our primary market.\n",
    "Customer churn decreased to 5% from 8%.\n",
    "New user acquisition cost is $43 per user.\n",
    "Product adoption rate increased to 78%.\n",
    "Employee satisfaction is at 87 points.\n",
    "Operating margin improved to 34%.\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\nInput text:\")\n",
    "print(report)\n",
    "formatted_result = chain(report, data_processing_steps, complete)\n",
    "print(formatted_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8bf1dfd-61f7-4d5c-afef-d2f04cf6845e",
   "metadata": {},
   "source": [
    "### Example 2: Parallelization\n",
    "\n",
    "Parallelization is effective when the divided subtasks can be parallelized for speed, or when multiple perspectives or attempts are needed for higher confidence results. For complex tasks with multiple considerations, LLMs generally perform better when each consideration is handled by a separate LLM call, allowing focused attention on each specific aspect.\n",
    "\n",
    "In this example, try applying `func` per `something`.\n",
    "\n",
    "Example:\n",
    "- Create sports news letter article for all games played today.\n",
    "- Create .... x per y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4e6f78f8-b97b-4cde-97e1-7ecbafef6602",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapped(prompt, inputs: List[str], llm_fn):\n",
    "    raise NotImplementedError(\"fill me\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbedf9bd-4d60-4953-be0f-c358de799128",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_call(inp):\n",
    "    sys_prompt = \"You are intelligent marketer!\"\n",
    "    return complete(inp, sys_prompt)\n",
    "\n",
    "task_prompt = \"\" # fill me\n",
    "\n",
    "results = mapped(\n",
    "    task_prompt,\n",
    "    [\n",
    "        \"\" # fill me\n",
    "        \"\", # fill me\n",
    "    ],\n",
    "    llm_call\n",
    ")\n",
    "\n",
    "for result in results:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783d46b2-4923-478b-9406-9a49ed55ad3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
